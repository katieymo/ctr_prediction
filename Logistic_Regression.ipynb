{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import ast\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "import sys\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, OneHotEncoderModel\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, StandardScaler, Imputer, VectorAssembler, SQLTransformer\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"Final_Project\")\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "app_name = \"final_project_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://docker.w261:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Final_Project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc276492978>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.id', 'local-1576041156192'),\n",
       " ('spark.app.name', 'final_project_notebook'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.port', '44759'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.host', 'docker.w261')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "toyDF = spark.read.csv(\"toy_example.txt\", header=True)\n",
    "toyRDD = toyDF.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+-----+-----+--------+--------+--------+--------+\n",
      "|  y|  x1|   x2|   x6|   x8|     x14|     x19|     x20|     x35|\n",
      "+---+----+-----+-----+-----+--------+--------+--------+--------+\n",
      "|1.0| 2.0|671.0|145.0| 12.0|05db9164|fbad5c96|6c5e14ec|    null|\n",
      "|0.0| 0.0| -1.0|  9.0|  0.0|05db9164|6f6d9be8|2f5788d6|    null|\n",
      "|0.0|null|  0.0|100.0|  0.0|8cf07265|7e0ccccf|2cc59e2b|ad3062eb|\n",
      "|0.0|null| -1.0| null|  0.0|05db9164|fbad5c96|d356c7e6|    null|\n",
      "|0.0|null|  1.0|203.0|  5.0|68fd1e64|fbad5c96|d5f62b87|    null|\n",
      "|0.0| 1.0| -1.0|  0.0|  0.0|05db9164|    null|1b76cf1e|    null|\n",
      "|1.0|null| 39.0| null|117.0|5bfa8ab5|7e0ccccf|af0809a5|    null|\n",
      "|0.0|null|  0.0| 66.0|  7.0|05db9164|    null|da33ebe6|    null|\n",
      "|1.0|10.0|  1.0| 66.0| 27.0|05db9164|fbad5c96|ce4f7f55|    null|\n",
      "|0.0|null|  1.0| 16.0|  7.0|68fd1e64|7e0ccccf|5e64ce5f|    null|\n",
      "+---+----+-----+-----+-----+--------+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "toyDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our EDA, we decided to convert numeric features into categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricalToImpute = toyDF.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputeValues(df, categoricalToImpute):\n",
    "    \n",
    "    # Impute categorical features\n",
    "    for col in categoricalToImpute:\n",
    "        mostCommon = df.select(col).groupby(col).count()\\\n",
    "                            .orderBy('count', ascending=False) \\\n",
    "                            .limit(1).collect()[0][0]\n",
    "        if mostCommon == \"\" or mostCommon is None:\n",
    "            mostCommon = \"EMPTY\"\n",
    "        \n",
    "        print(f\"Column {col} has most common {mostCommon}\")\n",
    "        \n",
    "        df = df.withColumn(col, F.when((df[col].isNull() | (df[col] == '')), mostCommon) \\\n",
    "                                .otherwise(df[col]))\n",
    "\n",
    "    return (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column x1 has most common EMPTY\n",
      "Column x2 has most common 1.0\n",
      "Column x6 has most common EMPTY\n",
      "Column x8 has most common 0.0\n",
      "Column x14 has most common 05db9164\n",
      "Column x19 has most common fbad5c96\n",
      "Column x20 has most common d356c7e6\n",
      "Column x35 has most common EMPTY\n"
     ]
    }
   ],
   "source": [
    "toyDF = imputeValues(toyDF,categoricalToImpute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After imputation, we can see that the null values have converted a new string value 'EMPTY' for columns x1 and x35, as the null value was the most common value in those features. The null values originally present in columns x6 and x19 have been imputed to the most common value, 66.0 and fbad5c96, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+-----+-----+--------+--------+--------+--------+\n",
      "|  y|   x1|   x2|   x6|   x8|     x14|     x19|     x20|     x35|\n",
      "+---+-----+-----+-----+-----+--------+--------+--------+--------+\n",
      "|1.0|  2.0|671.0|145.0| 12.0|05db9164|fbad5c96|6c5e14ec|   EMPTY|\n",
      "|0.0|  0.0| -1.0|  9.0|  0.0|05db9164|6f6d9be8|2f5788d6|   EMPTY|\n",
      "|0.0|EMPTY|  0.0|100.0|  0.0|8cf07265|7e0ccccf|2cc59e2b|ad3062eb|\n",
      "|0.0|EMPTY| -1.0|EMPTY|  0.0|05db9164|fbad5c96|d356c7e6|   EMPTY|\n",
      "|0.0|EMPTY|  1.0|203.0|  5.0|68fd1e64|fbad5c96|d5f62b87|   EMPTY|\n",
      "|0.0|  1.0| -1.0|  0.0|  0.0|05db9164|fbad5c96|1b76cf1e|   EMPTY|\n",
      "|1.0|EMPTY| 39.0|EMPTY|117.0|5bfa8ab5|7e0ccccf|af0809a5|   EMPTY|\n",
      "|0.0|EMPTY|  0.0| 66.0|  7.0|05db9164|fbad5c96|da33ebe6|   EMPTY|\n",
      "|1.0| 10.0|  1.0| 66.0| 27.0|05db9164|fbad5c96|ce4f7f55|   EMPTY|\n",
      "|0.0|EMPTY|  1.0| 16.0|  7.0|68fd1e64|7e0ccccf|5e64ce5f|   EMPTY|\n",
      "+---+-----+-----+-----+-----+--------+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "toyDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once imputed, the categorical features will need to get one hot encoded. For example, column x1 has 5 unique values: 0.0, 1.0, 2.0, 10.0, and EMPTY. This one column will then get mapped to 4 columns, such the first column would be an indicator of whether the value is 0.0, the second column would be an indicator of whether the value is 1.0, and so on. The last value is omitted as it does not provide any additional value. Thus, a value of EMPTY would have 0 values for all 4 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns= categoricalToImpute\n",
    "\n",
    "# The index of string values multiple columns\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c))\n",
    "    for c in categorical_columns\n",
    "]\n",
    "\n",
    "# The encode of indexed values multiple columns\n",
    "encoders = [OneHotEncoder(dropLast=False,inputCol=indexer.getOutputCol(),\n",
    "            outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) \n",
    "    for indexer in indexers\n",
    "]\n",
    "\n",
    "# Vectorizing encoded values\n",
    "assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders],outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=indexers + encoders+[assembler])\n",
    "model=pipeline.fit(toyDF)\n",
    "transformed = model.transform(toyDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(y='1.0', x1='2.0', x2='671.0', x6='145.0', x8='12.0', x14='05db9164', x19='fbad5c96', x20='6c5e14ec', x35='EMPTY', x1_indexed=3.0, x2_indexed=4.0, x6_indexed=6.0, x8_indexed=2.0, x14_indexed=0.0, x19_indexed=0.0, x20_indexed=5.0, x35_indexed=0.0, x1_indexed_encoded=SparseVector(5, {3: 1.0}), x2_indexed_encoded=SparseVector(5, {4: 1.0}), x6_indexed_encoded=SparseVector(8, {6: 1.0}), x8_indexed_encoded=SparseVector(6, {2: 1.0}), x14_indexed_encoded=SparseVector(4, {0: 1.0}), x19_indexed_encoded=SparseVector(3, {0: 1.0}), x20_indexed_encoded=SparseVector(10, {5: 1.0}), x35_indexed_encoded=SparseVector(2, {0: 1.0}), features=SparseVector(43, {3: 1.0, 9: 1.0, 16: 1.0, 20: 1.0, 24: 1.0, 28: 1.0, 36: 1.0, 41: 1.0}))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the number of unique values for each column\n",
    "uniqueVals = []\n",
    "for i, col in enumerate(transformed.columns[1:9]):\n",
    "    uniqueVals.append(len(transformed.select(col).groupby(col).count().distinct().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import lit, udf\n",
    "\n",
    "def ith_(v, i):\n",
    "    try:\n",
    "        return float(v[i])\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "ith = udf(ith_, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the indexed_encoded columns, create one hot encoded columns \n",
    "for i, col in enumerate(transformed.columns[-9:-1]):\n",
    "    for j in range(uniqueVals[i]-1):\n",
    "        transformed = transformed.withColumn(f'oh_{i}_{j}', ith(col, lit(j)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove features so that only one hot encoded features are left\n",
    "for col in transformed.columns[1:26]:\n",
    "    transformed = transformed.drop(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then add a bias term to our data with a value of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = transformed.withColumn('x0', lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "toyRDD = transformed.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have converted our toy example CSV into something we can perform gradient descent using map reduce paradigm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(y='1.0', oh_0_0=0.0, oh_0_1=0.0, oh_0_2=0.0, oh_0_3=1.0, oh_1_0=0.0, oh_1_1=0.0, oh_1_2=0.0, oh_1_3=0.0, oh_2_0=0.0, oh_2_1=0.0, oh_2_2=0.0, oh_2_3=0.0, oh_2_4=0.0, oh_2_5=0.0, oh_2_6=1.0, oh_3_0=0.0, oh_3_1=0.0, oh_3_2=1.0, oh_3_3=0.0, oh_3_4=0.0, oh_4_0=1.0, oh_4_1=0.0, oh_4_2=0.0, oh_5_0=1.0, oh_5_1=0.0, oh_6_0=0.0, oh_6_1=0.0, oh_6_2=0.0, oh_6_3=0.0, oh_6_4=0.0, oh_6_5=1.0, oh_6_6=0.0, oh_6_7=0.0, oh_6_8=0.0, oh_7_0=1.0, x0=1)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toyRDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Gradient Descent with Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform gradient descent with logistic regression, we defined five helper functions:\n",
    "\n",
    "1. The predict function will return a probability for each observation by taking the dot product of the features and the weights, as defined by the linear model, $ y = \\theta_0 + \\theta_1 x + ... + \\theta_l x_l = \\theta^T x$. \n",
    "2. Then the sigmoid function maps that output to a probability value between 0 and 1 for class 1: $ g(z) = \\frac{1}{1+e^{-z}} $. \n",
    "3. The calcCost functions calculates the log loss $ L(\\hat{y},y) = -(ylog\\hat{y} + (1-y)log(1-\\hat{y})) $. The first part of the equation $-ylog\\hat{y}$ is the cost when $y=1$ and the second part of the equation $-(1-y)log(1-\\hat{y})$ is the cost when $y=0$. In the case of training with class weights for imbalanced data, the log loss becomes $ L(\\hat{y},y) = -(rylog\\hat{y} + (1-r)(1-y)log(1-\\hat{y})) $, where $r$ is the balancing ratio. The balancing ratio is the ratio of the number of observations in the training data in class 0 over the total number of observations. In our click through rate data set, we have an imbalance with class 0 as the majority class. Thus, the observations in class 1 will be penalized more if classified incorrectly.\n",
    "4. The gradients function calculates the gradient to be subtracted from the weights at each iteration. The gradient is the derivative of the loss function and is simplified to $ L' = x(\\hat{y}-y)$, where $\\hat{y}$ is the prediction and $y$ is the actual class label. In the case of training with class weights, the derivative becomes $ L' = x[(1-r)(1-y)\\hat{y} - (ry\\hat{y})]$. \n",
    "5. Finally, the decision function takes a threshold, typically 0.5, and predicts class 1 if the probability is greater or equal than the threshold, or predicts class 0 if the probability is lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html\n",
    "http://www.scalaformachinelearning.com/2016/03/weighting-logistic-loss-for-imbalanced_7.html\n",
    "https://stats.stackexchange.com/questions/278771/how-is-the-cost-function-from-logistic-regression-derivated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes a prediction by taking the dot product of the values and the weights and passes it through the sigmoid function\n",
    "def predict(weights, line):\n",
    "    y = np.array(line[0]).astype(dtype='float')\n",
    "    x = np.array(line[1:]).astype(dtype='float')\n",
    "    z = np.dot(x, weights)\n",
    "    \n",
    "    return sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients(weights, lr, balancingRatio, line):\n",
    "    y = np.array(line[0]).astype(dtype='float')\n",
    "    x = np.array(line[1:]).astype(dtype='float')\n",
    "    z = np.dot(x, weights)\n",
    "    \n",
    "    predictions = sigmoid(z)\n",
    "    if balancingRatio:\n",
    "        gradient = np.dot(x.T, (1-balancingRatio)*(1-y)*predictions - (balancingRatio*y*(1-predictions)))\n",
    "    else:\n",
    "        gradient = np.dot(x.T, predictions - y)\n",
    "    gradient = gradient/len(theta)\n",
    "    gradient = gradient*lr\n",
    "    \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcCost(weights, eps, balancingRatio, line):\n",
    "    y = np.array(line[0]).astype(dtype='float')\n",
    "    x = np.array(line[1:]).astype(dtype='float')\n",
    "    z = np.dot(x, weights)\n",
    "    pred = sigmoid(z)\n",
    "    \n",
    "    #Take the error when label=1\n",
    "    class1_cost = -y*np.log(pred+eps) # Add epsilon to prevent nan\n",
    "\n",
    "    #Take the error when label=0\n",
    "    class0_cost = (1-y)*np.log(1-pred+eps) # Add epsilon to prevent nan\n",
    "    \n",
    "    #If training with class weights, multiply costs by the balancing ratio (majority class 0/total observations)\n",
    "    if balancingRatio:\n",
    "        class1_cost *= balancingRatio\n",
    "        class0_cost *= (1-balancingRatio)\n",
    "    \n",
    "    cost = class1_cost - class0_cost\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(toyRDD, theta, ITERATIONS, LR, EPSILON, verbose=False, reg_param=0, class_weights=False):\n",
    "    cost_history = []\n",
    "    \n",
    "    if class_weights == True:\n",
    "        total_count = toyRDD.count()\n",
    "        class0_count = toyRDD.map(lambda x: x[0]).filter(lambda x: x==\"0.0\").count()\n",
    "        balancingRatio = class0_count/total_count\n",
    "    else:\n",
    "        balancingRatio = None\n",
    "\n",
    "    for i in range(ITERATIONS+1):\n",
    "        \n",
    "        preds = toyRDD.map(partial(predict, theta)).collect()\n",
    "        preds = [\"%.2f\" % v for v in preds] # Format to 2 decimals\n",
    "        \n",
    "        grads = toyRDD.map(partial(gradients, theta, LR, balancingRatio)).mean() + reg_param*2*np.append(theta[:-1],[0.0]) # Add in Regularization\n",
    "        \n",
    "        theta = theta - grads\n",
    "        \n",
    "        #Calculate Error for Tracking\n",
    "        cost = toyRDD.map(partial(calcCost, theta, EPSILON, balancingRatio)).mean()\n",
    "        cost_history.append(cost)\n",
    "\n",
    "        # Log Progress\n",
    "        if verbose:\n",
    "            if i % 50 == 0:\n",
    "                print(f\"==========================================================\")\n",
    "                print(f\"Iter: {i}, Cost: {cost:.3}\")\n",
    "                print(f\"==========================================================\")\n",
    "                print(f\"Preds: {preds}\")\n",
    "                print(f\"==========================================================\")\n",
    "                print(f\"Weights: {theta}\")\n",
    "\n",
    "    return theta, cost_history, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision(preds, threshold):\n",
    "    outcome = ['1.0' if float(x) >= threshold else '0.0' for x in preds]\n",
    "    return outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent on Toy Example with no Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here We Created Our Weight Matrix Initialized to 0's\n",
    "theta = np.zeros(len(transformed.columns)-1).astype(dtype='float')\n",
    "LR = 0.5\n",
    "ITERATIONS = 1000\n",
    "EPSILON = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "Iter: 0, Cost: 0.689\n",
      "==========================================================\n",
      "Preds: ['0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50']\n",
      "==========================================================\n",
      "Weights: [-0.00277778 -0.00069444  0.00069444  0.00069444 -0.00208333 -0.00069444\n",
      " -0.00138889  0.00069444  0.          0.         -0.00069444 -0.00069444\n",
      " -0.00069444 -0.00069444  0.00069444 -0.00277778 -0.00138889  0.00069444\n",
      "  0.00069444  0.00069444 -0.00138889 -0.00138889 -0.00069444 -0.00138889\n",
      " -0.00069444 -0.00069444 -0.00069444 -0.00069444  0.00069444  0.00069444\n",
      "  0.00069444 -0.00069444 -0.00069444 -0.00069444 -0.00208333 -0.00277778]\n",
      "==========================================================\n",
      "Iter: 50, Cost: 0.559\n",
      "==========================================================\n",
      "Preds: ['0.47', '0.38', '0.38', '0.35', '0.38', '0.37', '0.45', '0.38', '0.46', '0.37']\n",
      "==========================================================\n",
      "Weights: [-0.11495621 -0.03079023  0.03730521  0.03680068 -0.09075631 -0.02410253\n",
      " -0.06133872  0.03741829  0.00663218  0.00779024 -0.0306657  -0.03085775\n",
      " -0.03033804 -0.03054998  0.03680068 -0.12142201 -0.06122301  0.03680068\n",
      "  0.03730521  0.03741829 -0.04732346 -0.06140773 -0.0306657  -0.04739098\n",
      " -0.02379739 -0.03079023 -0.02962804 -0.03033804  0.03730521  0.03741829\n",
      "  0.03680068 -0.03054998 -0.0306657  -0.03085775 -0.0713129  -0.1019786 ]\n",
      "==========================================================\n",
      "Iter: 100, Cost: 0.485\n",
      "==========================================================\n",
      "Preds: ['0.48', '0.31', '0.30', '0.28', '0.32', '0.30', '0.44', '0.32', '0.46', '0.30']\n",
      "==========================================================\n",
      "Weights: [-0.19256643 -0.05458974  0.07514473  0.07346306 -0.15898379 -0.03371829\n",
      " -0.10844625  0.07586379  0.02053491  0.02474285 -0.05383643 -0.05500941\n",
      " -0.05327311 -0.05385361  0.07346306 -0.21282022 -0.10846344  0.07346306\n",
      "  0.07514473  0.07586379 -0.06498582 -0.10886303 -0.05383643 -0.06540549\n",
      " -0.03182625 -0.05458974 -0.05112094 -0.05327311  0.07514473  0.07586379\n",
      "  0.07346306 -0.05385361 -0.05383643 -0.05500941 -0.09798505 -0.15182148]\n",
      "==========================================================\n",
      "Iter: 150, Cost: 0.432\n",
      "==========================================================\n",
      "Preds: ['0.50', '0.27', '0.25', '0.23', '0.28', '0.26', '0.46', '0.28', '0.48', '0.26']\n",
      "==========================================================\n",
      "Weights: [-0.25152062 -0.07461869  0.11215587  0.10889979 -0.21559851 -0.03690326\n",
      " -0.14798924  0.11412156  0.03683466  0.04552775 -0.07266803 -0.07573825\n",
      " -0.07238602 -0.07332088  0.10889979 -0.28826655 -0.14864209  0.10889979\n",
      "  0.11215587  0.11412156 -0.06986406 -0.14905913 -0.07266803 -0.07098362\n",
      " -0.03186736 -0.07461869 -0.0685938  -0.07238602  0.11215587  0.11412156\n",
      "  0.10889979 -0.07332088 -0.07266803 -0.07573825 -0.10480163 -0.17746967]\n",
      "==========================================================\n",
      "Iter: 200, Cost: 0.39\n",
      "==========================================================\n",
      "Preds: ['0.53', '0.24', '0.21', '0.20', '0.26', '0.23', '0.47', '0.26', '0.51', '0.23']\n",
      "==========================================================\n",
      "Weights: [-0.29955293 -0.09214431  0.14745809  0.14237527 -0.26483452 -0.03712573\n",
      " -0.18262687  0.1512875   0.05335615  0.06765775 -0.08852493 -0.09428385\n",
      " -0.08906046 -0.09029996  0.14237527 -0.35335945 -0.1844019   0.14237527\n",
      "  0.14745809  0.1512875  -0.0691031  -0.18458381 -0.08852493 -0.07124264\n",
      " -0.0275374  -0.09214431 -0.08362975 -0.08906046  0.14745809  0.1512875\n",
      "  0.14237527 -0.09029996 -0.08852493 -0.09428385 -0.10239942 -0.19092435]\n",
      "==========================================================\n",
      "Iter: 250, Cost: 0.354\n",
      "==========================================================\n",
      "Preds: ['0.57', '0.22', '0.18', '0.18', '0.24', '0.21', '0.50', '0.24', '0.54', '0.21']\n",
      "==========================================================\n",
      "Weights: [-0.34062924 -0.10784595  0.18072505  0.17365292 -0.30885855 -0.03605108\n",
      " -0.2137818   0.18693788  0.06914981  0.08992869 -0.10220657 -0.11127386\n",
      " -0.1040034  -0.10550227  0.17365292 -0.41106511 -0.21707751  0.17365292\n",
      "  0.18072505  0.18693788 -0.06605582 -0.21677612 -0.10220657 -0.06948373\n",
      " -0.02077096 -0.10784595 -0.09700919 -0.1040034   0.18072505  0.18693788\n",
      "  0.17365292 -0.10550227 -0.10220657 -0.11127386 -0.09589407 -0.19810064]\n",
      "==========================================================\n",
      "Iter: 300, Cost: 0.324\n",
      "==========================================================\n",
      "Preds: ['0.60', '0.20', '0.16', '0.17', '0.22', '0.19', '0.52', '0.23', '0.57', '0.19']\n",
      "==========================================================\n",
      "Weights: [-0.37692485 -0.12212235  0.21190438  0.20274125 -0.34888727 -0.03448489\n",
      " -0.24227791  0.22089636  0.08384405  0.11174232 -0.11421758 -0.12705235\n",
      " -0.11761088 -0.11933692  0.20274125 -0.46310485 -0.24739725  0.20274125\n",
      "  0.21190438  0.22089636 -0.06230198 -0.24638926 -0.11421758 -0.06723198\n",
      " -0.01265813 -0.12212235 -0.10915405 -0.11761088  0.21190438  0.22089636\n",
      "  0.20274125 -0.11933692 -0.11421758 -0.12705235 -0.08779488 -0.20201246]\n",
      "==========================================================\n",
      "Iter: 350, Cost: 0.298\n",
      "==========================================================\n",
      "Preds: ['0.62', '0.18', '0.15', '0.15', '0.21', '0.17', '0.55', '0.22', '0.59', '0.18']\n",
      "==========================================================\n",
      "Weights: [-0.40971133 -0.13522982  0.2410772   0.22976406 -0.38567206 -0.03280873\n",
      " -0.26863107  0.25312065  0.09734493  0.13280568 -0.1248988  -0.14182529\n",
      " -0.13012726 -0.13206065  0.22976406 -0.51057085 -0.27579292  0.22976406\n",
      "  0.2410772   0.25312065 -0.05856307 -0.27388593 -0.1248988  -0.06515853\n",
      " -0.0038388  -0.13522982 -0.12031497 -0.13012726  0.2410772   0.25312065\n",
      "  0.22976406 -0.13206065 -0.1248988  -0.14182529 -0.07932835 -0.20422715]\n",
      "==========================================================\n",
      "Iter: 400, Cost: 0.275\n",
      "==========================================================\n",
      "Preds: ['0.65', '0.17', '0.13', '0.14', '0.19', '0.16', '0.57', '0.21', '0.62', '0.16']\n",
      "==========================================================\n",
      "Weights: [-0.4397782  -0.14734736  0.26838349  0.25489155 -0.41972044 -0.03119278\n",
      " -0.29318889  0.28364202  0.1096881   0.15298696 -0.1344935  -0.15572803\n",
      " -0.14171802 -0.14384823  0.25489155 -0.55421394 -0.30254362  0.25489155\n",
      "  0.26838349  0.28364202 -0.05514079 -0.29957627 -0.1344935  -0.06352146\n",
      "  0.00530029 -0.14734736 -0.13065506 -0.14171802  0.26838349  0.28364202\n",
      "  0.25489155 -0.14384823 -0.1344935  -0.15572803 -0.07107503 -0.20556854]\n",
      "==========================================================\n",
      "Iter: 450, Cost: 0.255\n",
      "==========================================================\n",
      "Preds: ['0.67', '0.16', '0.12', '0.13', '0.18', '0.15', '0.60', '0.20', '0.64', '0.15']\n",
      "==========================================================\n",
      "Weights: [-0.46764484 -0.15860867  0.29398248  0.2783044  -0.45140279 -0.02970277\n",
      " -0.31620129  0.31253124  0.12096414  0.1722417  -0.14318295 -0.16885794\n",
      " -0.15250459 -0.15482731  0.2783044  -0.59458574 -0.32784565  0.2783044\n",
      "  0.29398248  0.31253124 -0.05213426 -0.32368525 -0.14318295 -0.06238353\n",
      "  0.01452098 -0.15860867 -0.14028953 -0.15250459  0.29398248  0.31253124\n",
      "  0.2783044  -0.15482731 -0.14318295 -0.16885794 -0.06328827 -0.20647122]\n",
      "==========================================================\n",
      "Iter: 500, Cost: 0.237\n",
      "==========================================================\n",
      "Preds: ['0.70', '0.15', '0.11', '0.13', '0.17', '0.14', '0.62', '0.19', '0.66', '0.14']\n",
      "==========================================================\n",
      "Weights: [-0.49367198 -0.16911889  0.3180326   0.30017631 -0.48100622 -0.02835449\n",
      " -0.33785809  0.33987867  0.13128122  0.1905732  -0.15110671 -0.18129063\n",
      " -0.16258186 -0.16509645  0.30017631 -0.63211293 -0.35184783  0.30017631\n",
      "  0.3180326   0.33987867 -0.0495487  -0.34638708 -0.15110671 -0.06172044\n",
      "  0.02367551 -0.16911889 -0.14930548 -0.16258186  0.3180326   0.33987867\n",
      "  0.30017631 -0.16509645 -0.15110671 -0.18129063 -0.05605711 -0.20716382]\n",
      "==========================================================\n",
      "Iter: 550, Cost: 0.222\n",
      "==========================================================\n",
      "Preds: ['0.71', '0.14', '0.10', '0.12', '0.17', '0.13', '0.64', '0.19', '0.68', '0.13']\n",
      "==========================================================\n",
      "Weights: [-0.51812269 -0.17896377  0.34068212  0.32066641 -0.50876323 -0.02714125\n",
      " -0.35830981  0.36578266  0.14074729  0.20801049 -0.15837498 -0.19308834\n",
      " -0.17202729 -0.17473503  0.32066641 -0.66713822 -0.37466986  0.32066641\n",
      "  0.34068212  0.36578266 -0.04734953 -0.36782337 -0.15837498 -0.0614741\n",
      "  0.03267265 -0.17896377 -0.15777218 -0.17202729  0.34068212  0.36578266\n",
      "  0.32066641 -0.17473503 -0.15837498 -0.19308834 -0.04939023 -0.20776522]\n",
      "==========================================================\n",
      "Iter: 600, Cost: 0.208\n",
      "==========================================================\n",
      "Preds: ['0.73', '0.13', '0.09', '0.11', '0.16', '0.12', '0.66', '0.18', '0.70', '0.13']\n",
      "==========================================================\n",
      "Weights: [-0.54119714 -0.18821494  0.36206553  0.33991692 -0.53486779 -0.02604766\n",
      " -0.37767982  0.39034268  0.14946221  0.22459587 -0.1650765  -0.20430428\n",
      " -0.18090604 -0.18380891  0.33991692 -0.69994429 -0.39641223  0.33991692\n",
      "  0.36206553  0.39034268 -0.04548866 -0.38811319 -0.1650765  -0.061578\n",
      "  0.04145728 -0.18821494 -0.16574681 -0.18090604  0.36206553  0.39034268\n",
      "  0.33991692 -0.18380891 -0.1650765  -0.20430428 -0.04325917 -0.20833566]\n",
      "==========================================================\n",
      "Iter: 650, Cost: 0.196\n",
      "==========================================================\n",
      "Preds: ['0.75', '0.12', '0.09', '0.11', '0.15', '0.12', '0.67', '0.17', '0.72', '0.12']\n",
      "==========================================================\n",
      "Weights: [-0.56305305 -0.19693331  0.38230288  0.3580533  -0.55948501 -0.02505616\n",
      " -0.39607171  0.41365544  0.15751496  0.24037769 -0.17128379 -0.21498513\n",
      " -0.18927395 -0.19237391  0.3580533  -0.7307688  -0.41716183  0.3580533\n",
      "  0.38230288  0.41365544 -0.04391675 -0.40735904 -0.17128379 -0.06196857\n",
      "  0.04999774 -0.19693331 -0.17327774 -0.18927395  0.38230288  0.41365544\n",
      "  0.3580533  -0.19237391 -0.17128379 -0.21498513 -0.03762036 -0.20890414]\n",
      "==========================================================\n",
      "Iter: 700, Cost: 0.185\n",
      "==========================================================\n",
      "Preds: ['0.76', '0.12', '0.08', '0.10', '0.14', '0.11', '0.69', '0.17', '0.73', '0.11']\n",
      "==========================================================\n",
      "Weights: [-0.58381836 -0.20517127  0.40150042  0.37518558 -0.58275733 -0.02415003\n",
      " -0.41357405  0.43581273  0.1649833   0.25540614 -0.17705693 -0.22517242\n",
      " -0.19717948 -0.20047804  0.37518558 -0.75981426 -0.43699516  0.37518558\n",
      "  0.40150042  0.43581273 -0.04258846 -0.42565046 -0.17705693 -0.06258961\n",
      "  0.05827777 -0.20517127 -0.18040659 -0.19717948  0.40150042  0.43581273\n",
      "  0.37518558 -0.20047804 -0.17705693 -0.22517242 -0.03242618 -0.20948311]\n",
      "==========================================================\n",
      "Iter: 750, Cost: 0.174\n",
      "==========================================================\n",
      "Preds: ['0.77', '0.11', '0.08', '0.09', '0.14', '0.11', '0.70', '0.16', '0.74', '0.11']\n",
      "==========================================================\n",
      "Weights: [-0.6035992  -0.21297424  0.41975195  0.39141004 -0.60480881 -0.02331452\n",
      " -0.4302636   0.45690043  0.17193453  0.26973087 -0.18244619 -0.23490344\n",
      " -0.204665   -0.20816302  0.39141004 -0.787255   -0.45598044  0.39141004\n",
      "  0.41975195  0.45690043 -0.04146424 -0.44306647 -0.18244619 -0.06339343\n",
      "  0.06629122 -0.21297424 -0.18716956 -0.204665    0.41975195  0.45690043\n",
      "  0.39141004 -0.20816302 -0.18244619 -0.23490344 -0.02763027 -0.21007646]\n",
      "==========================================================\n",
      "Iter: 800, Cost: 0.165\n",
      "==========================================================\n",
      "Preds: ['0.78', '0.10', '0.07', '0.09', '0.13', '0.10', '0.72', '0.15', '0.76', '0.10']\n",
      "==========================================================\n",
      "Weights: [-0.62248526 -0.22038193  0.43714017  0.40681102 -0.62574822 -0.02253715\n",
      " -0.44620755  0.47699806  0.17842665  0.28339961 -0.18749404 -0.24421186\n",
      " -0.21176784 -0.21546545  0.40681102 -0.81324225 -0.47417897  0.40681102\n",
      "  0.43714017  0.47699806 -0.04051055 -0.45967732 -0.18749404 -0.06434048\n",
      "  0.07403857 -0.22038193 -0.19359845 -0.21176784  0.43714017  0.47699806\n",
      "  0.40681102 -0.21546545 -0.18749404 -0.24421186 -0.0231898  -0.21068384]\n",
      "==========================================================\n",
      "Iter: 850, Cost: 0.157\n",
      "==========================================================\n",
      "Preds: ['0.79', '0.10', '0.07', '0.09', '0.13', '0.09', '0.73', '0.15', '0.77', '0.10']\n",
      "==========================================================\n",
      "Weights: [-0.64055346 -0.22742915  0.45373814  0.42146247 -0.64567144 -0.02180762\n",
      " -0.46146518  0.49617881  0.18450958  0.29645749 -0.19223663 -0.25312818\n",
      " -0.21852096 -0.22241758  0.42146247 -0.83790807 -0.49164614  0.42146247\n",
      "  0.45373814  0.49617881 -0.03969939 -0.47554576 -0.19223663 -0.06539842\n",
      "  0.0815246  -0.22742915 -0.19972133 -0.21852096  0.45373814  0.49617881\n",
      "  0.42146247 -0.22241758 -0.19223663 -0.25312818 -0.01906634 -0.21130297]\n",
      "==========================================================\n",
      "Iter: 900, Cost: 0.15\n",
      "==========================================================\n",
      "Preds: ['0.80', '0.09', '0.06', '0.08', '0.12', '0.09', '0.74', '0.14', '0.78', '0.09']\n",
      "==========================================================\n",
      "Weights: [-0.65787053 -0.23414663  0.46961055  0.43542944 -0.66466337 -0.02111754\n",
      " -0.47608913  0.51450978  0.19022638  0.30894669 -0.19670496 -0.26168008\n",
      " -0.22495364 -0.22904801  0.43542944 -0.86136833 -0.50843217  0.43542944\n",
      "  0.46961055  0.51450978 -0.03900755 -0.49072809 -0.19670496 -0.06654101\n",
      "  0.08875681 -0.23414663 -0.2055631  -0.22495364  0.46961055  0.51450978\n",
      "  0.43542944 -0.22904801 -0.19670496 -0.26168008 -0.01522586 -0.21193082]\n",
      "==========================================================\n",
      "Iter: 950, Cost: 0.143\n",
      "==========================================================\n",
      "Preds: ['0.81', '0.09', '0.06', '0.08', '0.12', '0.09', '0.75', '0.14', '0.79', '0.09']\n",
      "==========================================================\n",
      "Weights: [-0.67449495 -0.24056157  0.4848148   0.44876933 -0.68279946 -0.02046014\n",
      " -0.49012637  0.53205232  0.19561424  0.32090636 -0.2009258  -0.26989276\n",
      " -0.23109192 -0.23538218  0.44876933 -0.88372526 -0.52458274  0.44876933\n",
      "  0.4848148   0.53205232 -0.03841589 -0.50527494 -0.2009258  -0.06774708\n",
      "  0.09574434 -0.24056157 -0.21114596 -0.23109192  0.4848148   0.53205232\n",
      "  0.44876933 -0.23538218 -0.2009258  -0.26989276 -0.01163851 -0.21256431]\n",
      "==========================================================\n",
      "Iter: 1000, Cost: 0.136\n",
      "==========================================================\n",
      "Preds: ['0.82', '0.09', '0.06', '0.08', '0.11', '0.08', '0.76', '0.13', '0.79', '0.09']\n",
      "==========================================================\n",
      "Weights: [-0.69047834 -0.24669816  0.49940204  0.46153296 -0.70014697 -0.01982997\n",
      " -0.50361902  0.54886245  0.2007054   0.33237268 -0.20492238 -0.27778917\n",
      " -0.23695905 -0.24144283  0.46153296 -0.90506935 -0.54013947  0.46153296\n",
      "  0.49940204  0.54886245 -0.0379086  -0.51923201 -0.20492238 -0.06899962\n",
      "  0.10249723 -0.24669816 -0.21648977 -0.23695905  0.49940204  0.54886245\n",
      "  0.46153296 -0.24144283 -0.20492238 -0.27778917 -0.00827816 -0.21320054]\n"
     ]
    }
   ],
   "source": [
    "theta, cost_history, preds = train(toyRDD, theta, ITERATIONS, LR, EPSILON, verbose=True, reg_param=0, class_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.0', '0.0', '0.0', '0.0', '0.0', '0.0', '1.0', '0.0', '1.0', '0.0']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcome = decision(preds, 0.5)\n",
    "outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.0', '0.0', '0.0', '0.0', '0.0', '0.0', '1.0', '0.0', '1.0', '0.0']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth = toyDF.select('y').rdd.map(lambda x: x[0]).collect()\n",
    "truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcome == truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 1000 iterations, we take the probability and compare it to the threshold of 0.5. If it is above 0.5, then we predict class 1, otherwise, we predict class 0. In this toy example, we find that our predictions matched the truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent on Toy Example with L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here We Created Our Weight Matrix Initialized to 0's\n",
    "theta = np.zeros(len(transformed.columns)-1).astype(dtype='float')\n",
    "LR = 0.5\n",
    "ITERATIONS = 1000\n",
    "EPSILON = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "Iter: 0, Cost: 0.689\n",
      "==========================================================\n",
      "Preds: ['0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50']\n",
      "==========================================================\n",
      "Weights: [-0.00277778 -0.00069444  0.00069444  0.00069444 -0.00208333 -0.00069444\n",
      " -0.00138889  0.00069444  0.          0.         -0.00069444 -0.00069444\n",
      " -0.00069444 -0.00069444  0.00069444 -0.00277778 -0.00138889  0.00069444\n",
      "  0.00069444  0.00069444 -0.00138889 -0.00138889 -0.00069444 -0.00138889\n",
      " -0.00069444 -0.00069444 -0.00069444 -0.00069444  0.00069444  0.00069444\n",
      "  0.00069444 -0.00069444 -0.00069444 -0.00069444 -0.00208333 -0.00277778]\n",
      "==========================================================\n",
      "Iter: 50, Cost: 0.559\n",
      "==========================================================\n",
      "Preds: ['0.47', '0.38', '0.38', '0.35', '0.38', '0.37', '0.45', '0.38', '0.46', '0.38']\n",
      "==========================================================\n",
      "Weights: [-0.11440197 -0.03064016  0.03711983  0.03661857 -0.0903163  -0.02398889\n",
      " -0.06104014  0.03723218  0.0065961   0.00774689 -0.03051642 -0.03070727\n",
      " -0.03019084 -0.03040145  0.03661857 -0.12083271 -0.06092517  0.03661857\n",
      "  0.03711983  0.03723218 -0.04710162 -0.06110872 -0.03051642 -0.04716873\n",
      " -0.02368568 -0.03064016 -0.02948529 -0.03019084  0.03711983  0.03723218\n",
      "  0.03661857 -0.03040145 -0.03051642 -0.03070727 -0.07097816 -0.10206393]\n",
      "==========================================================\n",
      "Iter: 100, Cost: 0.487\n",
      "==========================================================\n",
      "Preds: ['0.48', '0.31', '0.30', '0.28', '0.32', '0.30', '0.44', '0.32', '0.46', '0.31']\n",
      "==========================================================\n",
      "Weights: [-0.19076545 -0.05406895  0.07440604  0.07274619 -0.15747944 -0.03341949\n",
      " -0.10741369  0.07511584  0.02031701  0.02447377 -0.05332465 -0.05448364\n",
      " -0.05276842 -0.0533419   0.07274619 -0.21080409 -0.10743093  0.07274619\n",
      "  0.07440604  0.07511584 -0.06441624 -0.10782554 -0.05332465 -0.06483092\n",
      " -0.03155072 -0.05406895 -0.05064206 -0.05276842  0.07440604  0.07511584\n",
      "  0.07274619 -0.0533419  -0.05332465 -0.05448364 -0.09712594 -0.15230272]\n",
      "==========================================================\n",
      "Iter: 150, Cost: 0.435\n",
      "==========================================================\n",
      "Preds: ['0.50', '0.27', '0.25', '0.23', '0.28', '0.26', '0.45', '0.28', '0.48', '0.26']\n",
      "==========================================================\n",
      "Weights: [-0.24807052 -0.07356849  0.11052425  0.10733125 -0.21259315 -0.03643864\n",
      " -0.1459108   0.11245218  0.03626432  0.04480316 -0.07165087 -0.07466894\n",
      " -0.07137565 -0.07229394  0.10733125 -0.28424403 -0.14655386  0.10733125\n",
      "  0.11052425  0.11245218 -0.06899758 -0.14696288 -0.07165087 -0.07009803\n",
      " -0.03149264 -0.07356849 -0.06764902 -0.07137565  0.11052425  0.11245218\n",
      "  0.10733125 -0.07229394 -0.07165087 -0.07466894 -0.10350829 -0.17865072]\n",
      "==========================================================\n",
      "Iter: 200, Cost: 0.394\n",
      "==========================================================\n",
      "Preds: ['0.53', '0.24', '0.21', '0.21', '0.26', '0.23', '0.47', '0.26', '0.50', '0.23']\n",
      "==========================================================\n",
      "Weights: [-0.29416956 -0.09043919  0.14463933  0.13968661 -0.25998454 -0.03653035\n",
      " -0.17925346  0.14837168  0.05228317  0.06625357 -0.0868973  -0.09253186\n",
      " -0.08742724 -0.08863782  0.13968661 -0.34688184 -0.18099398  0.13968661\n",
      "  0.14463933  0.14837168 -0.06801476 -0.18116968 -0.0868973  -0.07010743\n",
      " -0.02716344 -0.09043919 -0.08211811 -0.08742724  0.14463933  0.14837168\n",
      "  0.13968661 -0.08863782 -0.0868973  -0.09253186 -0.10081276 -0.19302175]\n",
      "==========================================================\n",
      "Iter: 250, Cost: 0.359\n",
      "==========================================================\n",
      "Preds: ['0.56', '0.22', '0.19', '0.19', '0.24', '0.21', '0.50', '0.25', '0.53', '0.21']\n",
      "==========================================================\n",
      "Weights: [-0.33308633 -0.10538224  0.17647084  0.16962204 -0.30187977 -0.03535406\n",
      " -0.20890381  0.1824897   0.06745378  0.08764238 -0.09988675 -0.10871939\n",
      " -0.10165021 -0.10310551  0.16962204 -0.40176652 -0.21212256  0.16962204\n",
      "  0.17647084  0.1824897  -0.06480395 -0.2118249  -0.09988675 -0.06814109\n",
      " -0.02050256 -0.10538224 -0.09484732 -0.10165021  0.17647084  0.1824897\n",
      "  0.16962204 -0.10310551 -0.09988675 -0.10871939 -0.09413914 -0.20124928]\n",
      "==========================================================\n",
      "Iter: 300, Cost: 0.33\n",
      "==========================================================\n",
      "Preds: ['0.59', '0.20', '0.17', '0.17', '0.22', '0.19', '0.52', '0.24', '0.56', '0.20']\n",
      "==========================================================\n",
      "Weights: [-0.36702994 -0.11881259  0.20600654  0.19718505 -0.3395394  -0.03370536\n",
      " -0.23571436  0.21466764  0.081432    0.10839632 -0.11113983 -0.12359064\n",
      " -0.11445548 -0.11612126  0.19718505 -0.45067923 -0.2406958   0.19718505\n",
      "  0.20600654  0.21466764 -0.06092234 -0.2397119  -0.11113983 -0.06570038\n",
      " -0.01259346 -0.11881259 -0.10627132 -0.11445548  0.20600654  0.21466764\n",
      "  0.19718505 -0.11612126 -0.11113983 -0.12359064 -0.08596661 -0.20628707]\n",
      "==========================================================\n",
      "Iter: 350, Cost: 0.305\n",
      "==========================================================\n",
      "Preds: ['0.62', '0.19', '0.15', '0.16', '0.21', '0.18', '0.54', '0.22', '0.59', '0.18']\n",
      "==========================================================\n",
      "Weights: [-0.39729507 -0.13099934  0.23336036  0.22253036 -0.37374941 -0.03195748\n",
      " -0.26022354  0.2448971   0.0941457   0.1282463  -0.12100888 -0.13736407\n",
      " -0.12609928 -0.12795377  0.22253036 -0.49475829 -0.26716842  0.22253036\n",
      "  0.23336036  0.2448971  -0.05707334 -0.26531783 -0.12100888 -0.06343807\n",
      " -0.00406555 -0.13099934 -0.11665079 -0.12609928  0.23336036  0.2448971\n",
      "  0.22253036 -0.12795377 -0.12100888 -0.13736407 -0.07749408 -0.20966249]\n",
      "==========================================================\n",
      "Iter: 400, Cost: 0.284\n",
      "==========================================================\n",
      "Preds: ['0.64', '0.17', '0.14', '0.15', '0.20', '0.17', '0.57', '0.22', '0.61', '0.17']\n",
      "==========================================================\n",
      "Weights: [-0.42468896 -0.14213174  0.25869834  0.2458521  -0.4050454  -0.03027486\n",
      " -0.28279746  0.27323866  0.10564675  0.1470817  -0.12974587 -0.15018572\n",
      " -0.13675669 -0.13878748  0.2458521  -0.53479127 -0.29183907  0.2458521\n",
      "  0.25869834  0.27323866 -0.05354655 -0.2889732  -0.12974587 -0.06160052\n",
      "  0.00470531 -0.14213174 -0.12615697 -0.13675669  0.25869834  0.27323866\n",
      "  0.2458521  -0.13878748 -0.12974587 -0.15018572 -0.06928109 -0.21217459]\n",
      "==========================================================\n",
      "Iter: 450, Cost: 0.265\n",
      "==========================================================\n",
      "Preds: ['0.66', '0.16', '0.13', '0.14', '0.19', '0.16', '0.59', '0.21', '0.63', '0.16']\n",
      "==========================================================\n",
      "Weights: [-0.44974575 -0.15235193  0.28220035  0.26734945 -0.43382079 -0.02871984\n",
      " -0.30370166  0.29978787  0.11603746  0.1648761  -0.13753878 -0.16216214\n",
      " -0.14655708 -0.14875805  0.26734945 -0.57135957 -0.31492093  0.26734945\n",
      "  0.28220035  0.29978787 -0.05043387 -0.31092018 -0.13753878 -0.06024408\n",
      "  0.01349104 -0.15235193 -0.13491178 -0.14655708  0.28220035  0.29978787\n",
      "  0.26734945 -0.14875805 -0.13753878 -0.16216214 -0.06156619 -0.21424351]\n",
      "==========================================================\n",
      "Iter: 500, Cost: 0.248\n",
      "==========================================================\n",
      "Preds: ['0.68', '0.15', '0.12', '0.13', '0.18', '0.15', '0.61', '0.20', '0.65', '0.15']\n",
      "==========================================================\n",
      "Weights: [-0.47283888 -0.161772    0.30404121  0.2872106  -0.4603818  -0.02730636\n",
      " -0.32313925  0.32465583  0.1254345   0.18164793 -0.14453254 -0.17337686\n",
      " -0.15560191 -0.1579707   0.2872106  -0.60491434 -0.33657741  0.2872106\n",
      "  0.30404121  0.32465583 -0.0477367  -0.33134757 -0.14453254 -0.05934157\n",
      "  0.02215258 -0.161772   -0.14300789 -0.15560191  0.30404121  0.32465583\n",
      "  0.2872106  -0.1579707  -0.14453254 -0.17337686 -0.05442844 -0.21608999]\n",
      "==========================================================\n",
      "Iter: 550, Cost: 0.234\n",
      "==========================================================\n",
      "Preds: ['0.70', '0.15', '0.11', '0.13', '0.17', '0.14', '0.62', '0.19', '0.67', '0.14']\n",
      "==========================================================\n",
      "Weights: [-0.49424273 -0.17048347  0.32438247  0.30560623 -0.48497688 -0.02602692\n",
      " -0.3412722   0.34795816  0.13395197  0.19743886 -0.1508417  -0.18389896\n",
      " -0.1639741  -0.16651043  0.30560623 -0.63581858 -0.35694093  0.30560623\n",
      "  0.32438247  0.34795816 -0.04541868 -0.35040939 -0.1508417  -0.05883417\n",
      "  0.03060603 -0.17048347 -0.1505193  -0.1639741   0.32438247  0.34795816\n",
      "  0.30560623 -0.16651043 -0.1508417  -0.18389896 -0.04786991 -0.21782906]\n",
      "==========================================================\n",
      "Iter: 600, Cost: 0.221\n",
      "==========================================================\n",
      "Preds: ['0.72', '0.14', '0.10', '0.12', '0.17', '0.13', '0.64', '0.19', '0.69', '0.14']\n",
      "==========================================================\n",
      "Weights: [-0.51416756 -0.17856287  0.34336955  0.32268798 -0.50781347 -0.02486586\n",
      " -0.35823379  0.36980879  0.14169434  0.21230164 -0.15655859 -0.19378756\n",
      " -0.17174344 -0.17444785  0.32268798 -0.66437205 -0.37612305  0.32268798\n",
      "  0.34336955  0.36980879 -0.04343115 -0.3682354  -0.15655859 -0.05865584\n",
      "  0.03880236 -0.17856287 -0.15750715 -0.17174344  0.34336955  0.36980879\n",
      "  0.32268798 -0.17444785 -0.15655859 -0.19378756 -0.04185776 -0.21951949]\n",
      "==========================================================\n",
      "Iter: 650, Cost: 0.21\n",
      "==========================================================\n",
      "Preds: ['0.73', '0.13', '0.09', '0.11', '0.16', '0.13', '0.66', '0.18', '0.70', '0.13']\n",
      "==========================================================\n",
      "Weights: [-0.53278021 -0.18607527  0.36113163  0.33858915 -0.52906814 -0.02380566\n",
      " -0.37413631  0.39031657  0.14875411  0.22629338 -0.16175879 -0.20309446\n",
      " -0.17896968 -0.18184282  0.33858915 -0.69082693 -0.39422034  0.33858915\n",
      "  0.36113163  0.39031657 -0.04172488 -0.38493728 -0.16175879 -0.05874408\n",
      "  0.04671496 -0.18607527 -0.16402319 -0.17896968  0.36113163  0.39031657\n",
      "  0.33858915 -0.18184282 -0.16175879 -0.20309446 -0.0363456  -0.22119017]\n",
      "==========================================================\n",
      "Iter: 700, Cost: 0.199\n",
      "==========================================================\n",
      "Preds: ['0.74', '0.13', '0.09', '0.11', '0.15', '0.12', '0.67', '0.18', '0.71', '0.12']\n",
      "==========================================================\n",
      "Weights: [-0.55021687 -0.19307661  0.37778278  0.35342644 -0.54889322 -0.02282977\n",
      " -0.38907593  0.40958357  0.15521182  0.23947161 -0.16650497 -0.21186569\n",
      " -0.18570465 -0.18874686  0.35342644 -0.71539819 -0.41131781  0.35342644\n",
      "  0.37778278  0.40958357 -0.04025496 -0.40061255 -0.16650497 -0.05904404\n",
      "  0.05433174 -0.19307661 -0.17011196 -0.18570465  0.37778278  0.40958357\n",
      "  0.35342644 -0.18874686 -0.16650497 -0.21186569 -0.03128394 -0.2228542 ]\n",
      "==========================================================\n",
      "Iter: 750, Cost: 0.19\n",
      "==========================================================\n",
      "Preds: ['0.75', '0.12', '0.08', '0.10', '0.15', '0.12', '0.69', '0.17', '0.72', '0.12']\n",
      "==========================================================\n",
      "Weights: [-0.56659117 -0.19961546  0.39342351  0.36730189 -0.56742133 -0.0219237\n",
      " -0.40313617  0.42770442  0.16113703  0.2518922  -0.17084969 -0.22014247\n",
      " -0.19199365 -0.19520473  0.36730189 -0.73827102 -0.42749121  0.36730189\n",
      "  0.39342351  0.42770442 -0.03898241 -0.4153472  -0.17084969 -0.05950942\n",
      "  0.06165    -0.19961546 -0.17581222 -0.19199365  0.39342351  0.42770442\n",
      "  0.36730189 -0.19520473 -0.17084969 -0.22014247 -0.02662519 -0.22451651]\n",
      "==========================================================\n",
      "Iter: 800, Cost: 0.182\n",
      "==========================================================\n",
      "Preds: ['0.76', '0.11', '0.08', '0.10', '0.14', '0.11', '0.70', '0.17', '0.74', '0.11']\n",
      "==========================================================\n",
      "Weights: [-0.58199961 -0.20573423  0.40814242  0.38030488 -0.58476878 -0.0210752\n",
      " -0.41639022  0.44476623  0.16658959  0.26360823 -0.17483739 -0.22796194\n",
      " -0.19787656 -0.20125569  0.38030488 -0.75960617 -0.44280852  0.38030488\n",
      "  0.40814242  0.44476623 -0.03787431 -0.42921763 -0.17483739 -0.06010202\n",
      "  0.06867315 -0.20573423 -0.18115799 -0.19787656  0.40814242  0.44476623\n",
      "  0.38030488 -0.20125569 -0.17483739 -0.22796194 -0.02232571 -0.22617795]\n",
      "==========================================================\n",
      "Iter: 850, Cost: 0.174\n",
      "==========================================================\n",
      "Preds: ['0.77', '0.11', '0.08', '0.10', '0.14', '0.11', '0.71', '0.16', '0.75', '0.11']\n",
      "==========================================================\n",
      "Weights: [-0.59652532 -0.21147017  0.4220178   0.39251383 -0.60103814 -0.02027422\n",
      " -0.42890272  0.46084876  0.17162107  0.27466942 -0.17850599 -0.23535767\n",
      " -0.20338864 -0.20693435  0.39251383 -0.77954413 -0.45733109  0.39251383\n",
      "  0.4220178   0.46084876 -0.03690325 -0.44229202 -0.17850599 -0.06079075\n",
      "  0.07540842 -0.21147017 -0.18617933 -0.20338864  0.4220178   0.46084876\n",
      "  0.39251383 -0.20693435 -0.17850599 -0.23535767 -0.01834651 -0.22783746]\n",
      "==========================================================\n",
      "Iter: 900, Cost: 0.168\n",
      "==========================================================\n",
      "Preds: ['0.78', '0.11', '0.07', '0.09', '0.13', '0.10', '0.72', '0.16', '0.75', '0.11']\n",
      "==========================================================\n",
      "Weights: [-0.6102407  -0.21685616  0.43511897  0.40399779 -0.61632024 -0.01951256\n",
      " -0.44073116  0.47602492  0.17627589  0.285122   -0.18188807 -0.24236013\n",
      " -0.20856117 -0.2122714   0.40399779 -0.79820831 -0.47111449  0.40399779\n",
      "  0.43511897  0.47602492 -0.03604657 -0.45463154 -0.18188807 -0.06155055\n",
      "  0.08186544 -0.21685616 -0.19090292 -0.20856117  0.43511897  0.47602492\n",
      "  0.40399779 -0.2122714  -0.18188807 -0.24236013 -0.01465319 -0.22949323]\n",
      "==========================================================\n",
      "Iter: 950, Cost: 0.161\n",
      "==========================================================\n",
      "Preds: ['0.79', '0.10', '0.07', '0.09', '0.13', '0.10', '0.73', '0.15', '0.76', '0.10']\n",
      "==========================================================\n",
      "Weights: [-0.6232094  -0.22192134  0.44750754  0.41481778 -0.6306959  -0.01878364\n",
      " -0.45192688  0.49036119  0.18059249  0.29500866 -0.18501182 -0.24899699\n",
      " -0.21342203 -0.21729419  0.41481778 -0.81570773 -0.48420924  0.41481778\n",
      "  0.44750754  0.49036119 -0.03528563 -0.46629118 -0.18501182 -0.06236128\n",
      "  0.08805518 -0.22192134 -0.19535253 -0.21342203  0.44750754  0.49036119\n",
      "  0.41481778 -0.21729419 -0.18501182 -0.24899699 -0.01121562 -0.23114329]\n",
      "==========================================================\n",
      "Iter: 1000, Cost: 0.156\n",
      "==========================================================\n",
      "Preds: ['0.80', '0.10', '0.07', '0.09', '0.13', '0.10', '0.74', '0.15', '0.77', '0.10']\n",
      "==========================================================\n",
      "Weights: [-0.63548782 -0.22669166  0.45923844  0.4250279  -0.64423724 -0.01808215\n",
      " -0.46253597  0.50391821  0.18460424  0.30436875 -0.18790177 -0.25529344\n",
      " -0.21799611 -0.22202715  0.4250279  -0.83213901 -0.49666135  0.4250279\n",
      "  0.45923844  0.50391821 -0.0346051  -0.47732059 -0.18790177 -0.06320688\n",
      "  0.09398929 -0.22669166 -0.19954946 -0.21799611  0.45923844  0.50391821\n",
      "  0.4250279  -0.22202715 -0.18790177 -0.25529344 -0.00800748 -0.23278581]\n"
     ]
    }
   ],
   "source": [
    "theta_2, cost_history_2, prob_2 = train(toyRDD, theta, ITERATIONS, LR, EPSILON, verbose=True, reg_param=0.0001, class_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.0', '0.0', '0.0', '0.0', '0.0', '0.0', '1.0', '0.0', '1.0', '0.0']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcome = decision(preds, 0.5)\n",
    "outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcome == truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 1000 iterations, we take the probability and compare it to the threshold of 0.5. If it is above 0.5, then we predict class 1, otherwise, we predict class 0. In this toy example, we find that our predictions matched the truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent With Class Weighting of the Logistic Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here We Created Our Weight Matrix Initialized to 0's\n",
    "theta = np.zeros(len(transformed.columns)-1).astype(dtype='float')\n",
    "LR = 0.5\n",
    "ITERATIONS = 1000\n",
    "EPSILON = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "Iter: 0, Cost: 0.291\n",
      "==========================================================\n",
      "Preds: ['0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50', '0.50']\n",
      "==========================================================\n",
      "Weights: [-5.55555556e-04 -2.08333333e-04  4.86111111e-04  4.86111111e-04\n",
      " -6.25000000e-04  6.94444444e-05 -4.16666667e-04  4.86111111e-04\n",
      "  2.77777778e-04  2.77777778e-04 -2.08333333e-04 -2.08333333e-04\n",
      " -2.08333333e-04 -2.08333333e-04  4.86111111e-04 -8.33333333e-04\n",
      " -4.16666667e-04  4.86111111e-04  4.86111111e-04  4.86111111e-04\n",
      "  1.38888889e-04 -4.16666667e-04 -2.08333333e-04  1.38888889e-04\n",
      "  6.94444444e-05 -2.08333333e-04 -2.08333333e-04 -2.08333333e-04\n",
      "  4.86111111e-04  4.86111111e-04  4.86111111e-04 -2.08333333e-04\n",
      " -2.08333333e-04 -2.08333333e-04  2.08333333e-04 -2.43945489e-19]\n",
      "==========================================================\n",
      "Iter: 50, Cost: 0.273\n",
      "==========================================================\n",
      "Preds: ['0.53', '0.48', '0.47', '0.48', '0.49', '0.48', '0.52', '0.49', '0.53', '0.48']\n",
      "==========================================================\n",
      "Weights: [-0.0278569  -0.01042161  0.02410596  0.02406257 -0.03128963  0.00318188\n",
      " -0.02085309  0.02434972  0.01356819  0.01392028 -0.01031532 -0.01049321\n",
      " -0.01043858 -0.01043088  0.02406257 -0.04160495 -0.02096865  0.02406257\n",
      "  0.02410596  0.02434972  0.00634113 -0.02092409 -0.01031532  0.00626954\n",
      "  0.00360352 -0.01042161 -0.01042944 -0.01043858  0.02410596  0.02434972\n",
      "  0.02406257 -0.01043088 -0.01031532 -0.01049321  0.00976677 -0.00054855]\n",
      "==========================================================\n",
      "Iter: 100, Cost: 0.258\n",
      "==========================================================\n",
      "Preds: ['0.56', '0.46', '0.44', '0.46', '0.47', '0.46', '0.53', '0.48', '0.55', '0.46']\n",
      "==========================================================\n",
      "Weights: [-0.05417859 -0.02023641  0.0464736   0.04630148 -0.06079714  0.00568907\n",
      " -0.04051481  0.04738333  0.02579157  0.02712076 -0.01983278 -0.02051134\n",
      " -0.02029815 -0.02027319  0.04630148 -0.08062992 -0.04095522  0.04630148\n",
      "  0.0464736   0.04738333  0.01129591 -0.04078453 -0.01983278  0.01102098\n",
      "  0.00727736 -0.02023641 -0.02026257 -0.02029815  0.0464736   0.04738333\n",
      "  0.04630148 -0.02027319 -0.01983278 -0.02051134  0.01789471 -0.00193807]\n",
      "==========================================================\n",
      "Iter: 150, Cost: 0.243\n",
      "==========================================================\n",
      "Preds: ['0.58', '0.44', '0.42', '0.44', '0.46', '0.45', '0.55', '0.47', '0.57', '0.45']\n",
      "==========================================================\n",
      "Weights: [-0.07948428 -0.02965759  0.06771194  0.06732759 -0.08914951  0.00771677\n",
      " -0.05941612  0.069634    0.03708613  0.03992701 -0.02879032 -0.03025518\n",
      " -0.02978492 -0.02973999  0.06732759 -0.11793982 -0.06036579  0.06732759\n",
      "  0.06771194  0.069634    0.01526422 -0.05999517 -0.02879032  0.01466663\n",
      "  0.01110369 -0.02965759 -0.02970699 -0.02978492  0.06771194  0.069634\n",
      "  0.06732759 -0.02973999 -0.02879032 -0.03025518  0.02490305 -0.00388727]\n",
      "==========================================================\n",
      "Iter: 200, Cost: 0.23\n",
      "==========================================================\n",
      "Preds: ['0.60', '0.42', '0.39', '0.43', '0.45', '0.43', '0.56', '0.46', '0.59', '0.43']\n",
      "==========================================================\n",
      "Weights: [-0.10376965 -0.03869458  0.08792689  0.08725073 -0.11636533  0.00936441\n",
      " -0.07758026  0.09114126  0.04756704  0.05237309 -0.03722041 -0.03972228\n",
      " -0.03890258 -0.0388402   0.08725073 -0.15358575 -0.07920004  0.08725073\n",
      "  0.08792689  0.09114126  0.01845243 -0.07856248 -0.03722041  0.01742473\n",
      "  0.01508065 -0.03869458 -0.03876816 -0.03890258  0.08792689  0.09114126\n",
      "  0.08725073 -0.0388402  -0.03722041 -0.03972228  0.03103121 -0.0061892 ]\n",
      "==========================================================\n",
      "Iter: 250, Cost: 0.218\n",
      "==========================================================\n",
      "Preds: ['0.62', '0.41', '0.37', '0.41', '0.43', '0.41', '0.58', '0.45', '0.61', '0.41']\n",
      "==========================================================\n",
      "Weights: [-0.12705279 -0.04736003  0.10720982  0.10616744 -0.14247523  0.01071016\n",
      " -0.09503632  0.11193928  0.05733035  0.06448319 -0.04515685 -0.048914\n",
      " -0.04765912 -0.04758566  0.10616744 -0.18763208 -0.09746514  0.10616744\n",
      "  0.10720982  0.11193928  0.02102256 -0.09649966 -0.04515685  0.01946858\n",
      "  0.01919677 -0.04736003 -0.04745608 -0.04765912  0.10720982  0.11193928\n",
      "  0.10616744 -0.04758566 -0.04515685 -0.048914    0.03646217 -0.00869468]\n",
      "==========================================================\n",
      "Iter: 300, Cost: 0.207\n",
      "==========================================================\n",
      "Preds: ['0.64', '0.39', '0.35', '0.39', '0.42', '0.40', '0.59', '0.44', '0.63', '0.40']\n",
      "==========================================================\n",
      "Weights: [-0.14936668 -0.0556687   0.12563989  0.12416277 -0.16751803  0.01181534\n",
      " -0.1118168   0.13205845  0.06645654  0.07627468 -0.05263345 -0.05783444\n",
      " -0.05606555 -0.05599012  0.12416277 -0.22015148 -0.11517347  0.12416277\n",
      "  0.12563989  0.13205845  0.02310128 -0.11382455 -0.05263345  0.02093554\n",
      "  0.02343489 -0.0556687  -0.05578378 -0.05606555  0.12563989  0.13205845\n",
      "  0.12416277 -0.05599012 -0.05263345 -0.05783444  0.04133518 -0.01129826]\n",
      "==========================================================\n",
      "Iter: 350, Cost: 0.197\n",
      "==========================================================\n",
      "Preds: ['0.66', '0.37', '0.33', '0.38', '0.41', '0.38', '0.61', '0.43', '0.64', '0.38']\n",
      "==========================================================\n",
      "Weights: [-0.17075343 -0.06363664  0.1432861   0.14131189 -0.19153778  0.01272811\n",
      " -0.12795579  0.15152656  0.07501351  0.08776035 -0.0596832  -0.06648963\n",
      " -0.06413492 -0.06406836  0.14131189 -0.25122097 -0.13234096  0.14131189\n",
      "  0.1432861   0.15152656  0.02478762 -0.13055799 -0.0596832   0.02193463\n",
      "  0.027775   -0.06363664 -0.06376622 -0.06413492  0.1432861   0.15152656\n",
      "  0.14131189 -0.06406836 -0.0596832  -0.06648963  0.04575619 -0.01392701]\n",
      "==========================================================\n",
      "Iter: 400, Cost: 0.187\n",
      "==========================================================\n",
      "Preds: ['0.67', '0.36', '0.31', '0.36', '0.40', '0.36', '0.62', '0.42', '0.66', '0.37']\n",
      "==========================================================\n",
      "Weights: [-0.19126021 -0.07128049  0.160209    0.1576815  -0.21458143  0.01348639\n",
      " -0.1434877   0.17036955  0.08305896  0.09895011 -0.06633766 -0.07488692\n",
      " -0.0718815  -0.07183569  0.1576815  -0.28091909 -0.14898573  0.1576815\n",
      "  0.160209    0.17036955  0.02615903 -0.14672261 -0.06633766  0.02255261\n",
      "  0.03219619 -0.07128049 -0.07141944 -0.0718815   0.160209    0.17036955\n",
      "  0.1576815  -0.07183569 -0.06633766 -0.07488692  0.04980597 -0.01653169]\n",
      "==========================================================\n",
      "Iter: 450, Cost: 0.179\n",
      "==========================================================\n",
      "Preds: ['0.68', '0.35', '0.29', '0.35', '0.39', '0.35', '0.63', '0.41', '0.67', '0.35']\n",
      "==========================================================\n",
      "Weights: [-0.21093619 -0.07861703  0.17646211  0.1733311  -0.23669714  0.01412016\n",
      " -0.15844634  0.18861204  0.09064238  0.1098521  -0.07262661 -0.08303451\n",
      " -0.07932017 -0.07930745  0.1733311  -0.30932375 -0.16512718  0.1733311\n",
      "  0.17646211  0.18861204  0.02727635 -0.16234195 -0.07262661  0.02285887\n",
      "  0.03667799 -0.07861703 -0.07875994 -0.07932017  0.17646211  0.18861204\n",
      "  0.1733311  -0.07930745 -0.07262661 -0.08303451  0.05354644 -0.01908017]\n",
      "==========================================================\n",
      "Iter: 500, Cost: 0.171\n",
      "==========================================================\n",
      "Preds: ['0.70', '0.33', '0.28', '0.33', '0.37', '0.34', '0.64', '0.40', '0.68', '0.34']\n",
      "==========================================================\n",
      "Weights: [-0.22983062 -0.08566278  0.19209305  0.18831399 -0.25793301  0.01465327\n",
      " -0.17286435  0.2062777   0.0978065   0.12047351 -0.07857781 -0.09094106\n",
      " -0.08646604 -0.08649872  0.18831399 -0.33651082 -0.18078526  0.18831399\n",
      "  0.19209305  0.2062777   0.02818748 -0.17743978 -0.07857781  0.02290921\n",
      "  0.04120118 -0.08566278 -0.08580419 -0.08646604  0.19209305  0.2062777\n",
      "  0.18831399 -0.08649872 -0.07857781 -0.09094106  0.05702541 -0.0215524 ]\n",
      "==========================================================\n",
      "Iter: 550, Cost: 0.163\n",
      "==========================================================\n",
      "Preds: ['0.71', '0.32', '0.26', '0.32', '0.36', '0.32', '0.65', '0.39', '0.70', '0.33']\n",
      "==========================================================\n",
      "Weights: [-0.24799144 -0.09243381  0.20714441  0.20267807 -0.27833619  0.01510476\n",
      " -0.18677286  0.22338938  0.10458855  0.13082106 -0.084217   -0.0986155\n",
      " -0.09333406 -0.09342415  0.20267807 -0.36255319 -0.19598001  0.20267807\n",
      "  0.20714441  0.22338938  0.02893043 -0.19203964 -0.084217    0.02274874\n",
      "  0.04574824 -0.09243381 -0.09256832 -0.09333406  0.20714441  0.22338938\n",
      "  0.20267807 -0.09342415 -0.084217   -0.0986155   0.06028016 -0.02393683]\n",
      "==========================================================\n",
      "Iter: 600, Cost: 0.156\n",
      "==========================================================\n",
      "Preds: ['0.72', '0.31', '0.25', '0.31', '0.35', '0.31', '0.66', '0.38', '0.71', '0.31']\n",
      "==========================================================\n",
      "Weights: [-0.26546451 -0.09894555  0.22165454  0.21646661 -0.29795232  0.01548997\n",
      " -0.20020126  0.23996925  0.11102115  0.14090132 -0.08956787 -0.10606677\n",
      " -0.09993883 -0.1000978   0.21646661 -0.38752019 -0.21073119  0.21646661\n",
      "  0.22165454  0.23996925  0.02953543 -0.20616457 -0.08956787  0.0224142\n",
      "  0.05030359 -0.09894555 -0.09906794 -0.09993883  0.22165454  0.23996925\n",
      "  0.21646661 -0.1000978  -0.08956787 -0.10606677  0.06334012 -0.02622775]\n",
      "==========================================================\n",
      "Iter: 650, Cost: 0.15\n",
      "==========================================================\n",
      "Preds: ['0.73', '0.30', '0.24', '0.29', '0.34', '0.30', '0.67', '0.37', '0.72', '0.30']\n",
      "==========================================================\n",
      "Weights: [-0.2822931  -0.10521269  0.23565811  0.22971876 -0.31682512  0.01582127\n",
      " -0.21317716  0.25603885  0.1171331   0.1507209  -0.09465216 -0.11330377\n",
      " -0.10629448 -0.10653307  0.22971876 -0.41147728 -0.22505807  0.22971876\n",
      "  0.23565811  0.25603885  0.03002675 -0.21983684 -0.09465216  0.02193567\n",
      "  0.05485362 -0.10521269 -0.10531795 -0.10629448  0.23565811  0.25603885\n",
      "  0.22971876 -0.10653307 -0.09465216 -0.11330377  0.06622876 -0.0284234 ]\n",
      "==========================================================\n",
      "Iter: 700, Cost: 0.144\n",
      "==========================================================\n",
      "Preds: ['0.74', '0.28', '0.23', '0.28', '0.33', '0.29', '0.68', '0.37', '0.73', '0.29']\n",
      "==========================================================\n",
      "Weights: [-0.29851771 -0.11124916  0.24918659  0.24247012 -0.33499617  0.01610873\n",
      " -0.22572638  0.27161905  0.12294998  0.16028653 -0.09948977 -0.12033518\n",
      " -0.11241449 -0.11274268  0.24247012 -0.43448594 -0.23897929  0.24247012\n",
      "  0.24918659  0.27161905  0.03042393 -0.23307786 -0.09948977  0.02133792\n",
      "  0.0593866  -0.11124916 -0.11133252 -0.11241449  0.24918659  0.27161905\n",
      "  0.24247012 -0.11274268 -0.09948977 -0.12033518  0.06896512 -0.03052465]\n",
      "==========================================================\n",
      "Iter: 750, Cost: 0.138\n",
      "==========================================================\n",
      "Preds: ['0.75', '0.27', '0.22', '0.27', '0.32', '0.28', '0.69', '0.36', '0.74', '0.28']\n",
      "==========================================================\n",
      "Weights: [-0.31417599 -0.11706809  0.26226868  0.25475309 -0.35250483  0.01636058\n",
      " -0.23787296  0.28673009  0.12849458  0.16960507 -0.10409886 -0.12716947\n",
      " -0.11831172 -0.11873863  0.25475309 -0.45660369 -0.25251273  0.25475309\n",
      "  0.26226868  0.28673009  0.03074284 -0.2459081  -0.10409886  0.02064146\n",
      "  0.0638926  -0.11706809 -0.11712502 -0.11831172  0.26226868  0.28673009\n",
      "  0.25475309 -0.11873863 -0.10409886 -0.12716947  0.07156483 -0.03253403]\n",
      "==========================================================\n",
      "Iter: 800, Cost: 0.133\n",
      "==========================================================\n",
      "Preds: ['0.76', '0.26', '0.21', '0.26', '0.31', '0.27', '0.70', '0.35', '0.74', '0.27']\n",
      "==========================================================\n",
      "Weights: [-0.32930283 -0.12268179  0.2749306   0.26659725 -0.36938817  0.01658356\n",
      " -0.2496393   0.30139155  0.13378733  0.17868352 -0.10849603 -0.13381481\n",
      " -0.12399834 -0.12453223  0.26659725 -0.47788421 -0.2656755   0.26659725\n",
      "  0.2749306   0.30139155  0.03099641 -0.25834704 -0.10849603  0.01986339\n",
      "  0.06836329 -0.12268179 -0.12270804 -0.12399834  0.2749306   0.30139155\n",
      "  0.26659725 -0.12453223 -0.10849603 -0.13381481  0.07404092 -0.03445511]\n",
      "==========================================================\n",
      "Iter: 850, Cost: 0.128\n",
      "==========================================================\n",
      "Preds: ['0.77', '0.26', '0.20', '0.25', '0.31', '0.26', '0.71', '0.34', '0.75', '0.26']\n",
      "==========================================================\n",
      "Weights: [-0.34393045 -0.12810183  0.28719642  0.27802968 -0.38568104  0.01678322\n",
      " -0.26104621  0.31562233  0.13884661  0.18752896 -0.1126964  -0.14027909\n",
      " -0.12948584 -0.13013411  0.27802968 -0.49837744 -0.27848391  0.27802968\n",
      "  0.28719642  0.31562233  0.03119525 -0.2704132  -0.1126964   0.019018\n",
      "  0.07279181 -0.12810183 -0.12809337 -0.12948584  0.28719642  0.31562233\n",
      "  0.27802968 -0.13013411 -0.1126964  -0.14027909  0.07640438 -0.03629202]\n",
      "==========================================================\n",
      "Iter: 900, Cost: 0.124\n",
      "==========================================================\n",
      "Preds: ['0.78', '0.25', '0.19', '0.25', '0.30', '0.25', '0.72', '0.33', '0.76', '0.26']\n",
      "==========================================================\n",
      "Weights: [-0.35808856 -0.13333901  0.29908824  0.28907519 -0.40141609  0.01696414\n",
      " -0.272113    0.3294406   0.14368902  0.19614855 -0.11671378 -0.14656985\n",
      " -0.13478502 -0.13555425  0.28907519 -0.51812987 -0.29095347  0.28907519\n",
      "  0.29908824  0.3294406   0.03134812 -0.2821241  -0.11671378  0.01811729\n",
      "  0.07717257 -0.13333901 -0.13329206 -0.13478502  0.29908824  0.3294406\n",
      "  0.28907519 -0.13555425 -0.11671378 -0.14656985  0.07866462 -0.03804916]\n",
      "==========================================================\n",
      "Iter: 950, Cost: 0.119\n",
      "==========================================================\n",
      "Preds: ['0.78', '0.24', '0.18', '0.24', '0.29', '0.24', '0.73', '0.33', '0.77', '0.25']\n",
      "==========================================================\n",
      "Weights: [-0.37180455 -0.13840343  0.31062645  0.2997566  -0.4166239   0.0171301\n",
      " -0.28285766  0.34286387  0.14832956  0.20454945 -0.12056077 -0.15269433\n",
      " -0.13990604 -0.14080202  0.2997566  -0.53718467 -0.3030989   0.2997566\n",
      "  0.31062645  0.34286387  0.03146226 -0.29349634 -0.12056077  0.01717137\n",
      "  0.08150108 -0.13840343 -0.13831443 -0.13990604  0.31062645  0.34286387\n",
      "  0.2997566  -0.14080202 -0.12056077 -0.15269433  0.08082979 -0.03973098]\n",
      "==========================================================\n",
      "Iter: 1000, Cost: 0.115\n",
      "==========================================================\n",
      "Preds: ['0.79', '0.23', '0.17', '0.23', '0.28', '0.23', '0.74', '0.32', '0.77', '0.24']\n",
      "==========================================================\n",
      "Weights: [-0.38510368 -0.14330452  0.32182987  0.31009488 -0.43133307  0.01728426\n",
      " -0.29329687  0.35590891  0.15278189  0.21273879 -0.12424888 -0.15865943\n",
      " -0.14485843 -0.14588618  0.31009488 -0.55558195 -0.31493416  0.31009488\n",
      "  0.32182987  0.35590891  0.0315437  -0.30454561 -0.12424888  0.0161888\n",
      "  0.08577385 -0.14330452 -0.14317012 -0.14485843  0.32182987  0.35590891\n",
      "  0.31009488 -0.14588618 -0.12424888 -0.15865943  0.08290701 -0.04134188]\n"
     ]
    }
   ],
   "source": [
    "theta, cost_history, preds = train(toyRDD, theta, ITERATIONS, LR, EPSILON, verbose=True, class_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.0', '0.0', '0.0', '0.0', '0.0', '0.0', '1.0', '0.0', '1.0', '0.0']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcome = decision(preds, 0.5)\n",
    "outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcome == truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we found that our predictions match the truth. The cost at the 1000 iteration was smaller than when training without class weights, but the probabilities are further away from 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
