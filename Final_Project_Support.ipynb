{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supporting Documentation & Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial conversion of CSV to Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import ast\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "import sys\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, OneHotEncoderModel\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, StandardScaler, MinMaxScaler, MaxAbsScaler, Imputer, VectorAssembler, SQLTransformer\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"Final_Project\")\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Column ID to testDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to submit to Kaggle we should ensure that the correct id is appended to the dataframe prior to processing with spark as we may lose the ability to know which row id we are evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "... Executed in 28.57644557952881 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "testDF = pd.read_csv(\"test.txt\", delimiter=\"\\t\", header=None)\n",
    "print(f\"\\n... Executed in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF.insert(0, \"id\", np.arange(len(testDF)) + 60000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF = testDF.astype({\"id\": int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "... Executed in 75.1240165233612 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "testDF.to_csv(\"test2.txt\", sep=\"\\t\", index=False, header=False)\n",
    "print(f\"\\n... Executed in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write the CSV's to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseAllColumns(line):\n",
    "    row = line.split(\"\\t\")\n",
    "    for i in range(14):\n",
    "        if row[i] == \"\":\n",
    "            row[i] = None\n",
    "        else:\n",
    "            row[i] = float(row[i])\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "... Executed in 450.37015986442566 seconds\n"
     ]
    }
   ],
   "source": [
    "# Convert train file from csv to parquet\n",
    "start = time.time()\n",
    "schema = StructType([\n",
    "    StructField(\"y\", FloatType(), True),\n",
    "    StructField(\"x1\", FloatType(), True),\n",
    "    StructField(\"x2\", FloatType(), True),\n",
    "    StructField(\"x3\", FloatType(), True),\n",
    "    StructField(\"x4\", FloatType(), True),\n",
    "    StructField(\"x5\", FloatType(), True),\n",
    "    StructField(\"x6\", FloatType(), True),\n",
    "    StructField(\"x7\", FloatType(), True),\n",
    "    StructField(\"x8\", FloatType(), True),\n",
    "    StructField(\"x9\", FloatType(), True),\n",
    "    StructField(\"x10\", FloatType(), True),\n",
    "    StructField(\"x11\", FloatType(), True),\n",
    "    StructField(\"x12\", FloatType(), True),\n",
    "    StructField(\"x13\", FloatType(), True),\n",
    "    StructField(\"x14\", StringType(), True),\n",
    "    StructField(\"x15\", StringType(), True),\n",
    "    StructField(\"x16\", StringType(), True),\n",
    "    StructField(\"x17\", StringType(), True),\n",
    "    StructField(\"x18\", StringType(), True),\n",
    "    StructField(\"x19\", StringType(), True),\n",
    "    StructField(\"x20\", StringType(), True),\n",
    "    StructField(\"x21\", StringType(), True),\n",
    "    StructField(\"x22\", StringType(), True),\n",
    "    StructField(\"x23\", StringType(), True),\n",
    "    StructField(\"x24\", StringType(), True),\n",
    "    StructField(\"x25\", StringType(), True),\n",
    "    StructField(\"x26\", StringType(), True),\n",
    "    StructField(\"x27\", StringType(), True),\n",
    "    StructField(\"x28\", StringType(), True),\n",
    "    StructField(\"x29\", StringType(), True),\n",
    "    StructField(\"x30\", StringType(), True),\n",
    "    StructField(\"x31\", StringType(), True),\n",
    "    StructField(\"x32\", StringType(), True),\n",
    "    StructField(\"x33\", StringType(), True),\n",
    "    StructField(\"x34\", StringType(), True),\n",
    "    StructField(\"x35\", StringType(), True),\n",
    "    StructField(\"x36\", StringType(), True),\n",
    "    StructField(\"x37\", StringType(), True),\n",
    "    StructField(\"x38\", StringType(), True),\n",
    "    StructField(\"x39\", StringType(), True)])\n",
    "\n",
    "rdd = sc.textFile('train.txt').map(parseAllColumns)\n",
    "df = sqlContext.createDataFrame(rdd, schema)\n",
    "df.write.parquet('train.parquet')\n",
    "print(f\"\\n... Executed in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "... Executed in 63.724459171295166 seconds\n"
     ]
    }
   ],
   "source": [
    "# Convert test file from csv to parquet\n",
    "start = time.time()\n",
    "schema = StructType([\n",
    "    StructField(\"id\", DoubleType(), True), # We Created an ID Label\n",
    "    StructField(\"x1\", FloatType(), True),\n",
    "    StructField(\"x2\", FloatType(), True),\n",
    "    StructField(\"x3\", FloatType(), True),\n",
    "    StructField(\"x4\", FloatType(), True),\n",
    "    StructField(\"x5\", FloatType(), True),\n",
    "    StructField(\"x6\", FloatType(), True),\n",
    "    StructField(\"x7\", FloatType(), True),\n",
    "    StructField(\"x8\", FloatType(), True),\n",
    "    StructField(\"x9\", FloatType(), True),\n",
    "    StructField(\"x10\", FloatType(), True),\n",
    "    StructField(\"x11\", FloatType(), True),\n",
    "    StructField(\"x12\", FloatType(), True),\n",
    "    StructField(\"x13\", FloatType(), True),\n",
    "    StructField(\"x14\", StringType(), True),\n",
    "    StructField(\"x15\", StringType(), True),\n",
    "    StructField(\"x16\", StringType(), True),\n",
    "    StructField(\"x17\", StringType(), True),\n",
    "    StructField(\"x18\", StringType(), True),\n",
    "    StructField(\"x19\", StringType(), True),\n",
    "    StructField(\"x20\", StringType(), True),\n",
    "    StructField(\"x21\", StringType(), True),\n",
    "    StructField(\"x22\", StringType(), True),\n",
    "    StructField(\"x23\", StringType(), True),\n",
    "    StructField(\"x24\", StringType(), True),\n",
    "    StructField(\"x25\", StringType(), True),\n",
    "    StructField(\"x26\", StringType(), True),\n",
    "    StructField(\"x27\", StringType(), True),\n",
    "    StructField(\"x28\", StringType(), True),\n",
    "    StructField(\"x29\", StringType(), True),\n",
    "    StructField(\"x30\", StringType(), True),\n",
    "    StructField(\"x31\", StringType(), True),\n",
    "    StructField(\"x32\", StringType(), True),\n",
    "    StructField(\"x33\", StringType(), True),\n",
    "    StructField(\"x34\", StringType(), True),\n",
    "    StructField(\"x35\", StringType(), True),\n",
    "    StructField(\"x36\", StringType(), True),\n",
    "    StructField(\"x37\", StringType(), True),\n",
    "    StructField(\"x38\", StringType(), True),\n",
    "    StructField(\"x39\", StringType(), True)])\n",
    "\n",
    "rdd = sc.textFile('test2.txt').map(parseAllColumns)\n",
    "df = sqlContext.createDataFrame(rdd, schema)\n",
    "df.write.parquet('test.parquet')\n",
    "print(f\"\\n... Executed in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "app_name = \"final_project_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://docker.w261:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Final_Project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7febf2e69a10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.memory', '25g'),\n",
       " ('spark.driver.port', '38519'),\n",
       " ('spark.app.name', 'final_project_notebook'),\n",
       " ('spark.app.id', 'local-1574524168976'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.host', 'docker.w261'),\n",
       " ('spark.executor.memory', '25g')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF = sqlContext.read.parquet('test.parquet') # This loads a Data Frame\n",
    "trainDF = sqlContext.read.parquet('train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADER = trainDF.columns\n",
    "INTEGER_FEATURES = HEADER[0:14] # These are the integer features\n",
    "CATEGORICAL_FEATURES = HEADER[14:] # These are the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = trainDF.sample(False, 0.0001, seed=1234).toPandas() # Approximately 5000 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_example = sample[5:15]\n",
    "toy_example.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_toy_example = toy_example[[\"y\",\"x1\",\"x2\",\"x6\",\"x8\",\"x14\",\"x19\",\"x20\",\"x35\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x6</th>\n",
       "      <th>x8</th>\n",
       "      <th>x14</th>\n",
       "      <th>x19</th>\n",
       "      <th>x20</th>\n",
       "      <th>x35</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>671.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>05db9164</td>\n",
       "      <td>fbad5c96</td>\n",
       "      <td>6c5e14ec</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>05db9164</td>\n",
       "      <td>6f6d9be8</td>\n",
       "      <td>2f5788d6</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8cf07265</td>\n",
       "      <td>7e0ccccf</td>\n",
       "      <td>2cc59e2b</td>\n",
       "      <td>ad3062eb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>05db9164</td>\n",
       "      <td>fbad5c96</td>\n",
       "      <td>d356c7e6</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>68fd1e64</td>\n",
       "      <td>fbad5c96</td>\n",
       "      <td>d5f62b87</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>05db9164</td>\n",
       "      <td></td>\n",
       "      <td>1b76cf1e</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>117.0</td>\n",
       "      <td>5bfa8ab5</td>\n",
       "      <td>7e0ccccf</td>\n",
       "      <td>af0809a5</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>05db9164</td>\n",
       "      <td></td>\n",
       "      <td>da33ebe6</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>05db9164</td>\n",
       "      <td>fbad5c96</td>\n",
       "      <td>ce4f7f55</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>68fd1e64</td>\n",
       "      <td>7e0ccccf</td>\n",
       "      <td>5e64ce5f</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y    x1     x2     x6     x8       x14       x19       x20       x35\n",
       "0  1.0   2.0  671.0  145.0   12.0  05db9164  fbad5c96  6c5e14ec          \n",
       "1  0.0   0.0   -1.0    9.0    0.0  05db9164  6f6d9be8  2f5788d6          \n",
       "2  0.0   NaN    0.0  100.0    0.0  8cf07265  7e0ccccf  2cc59e2b  ad3062eb\n",
       "3  0.0   NaN   -1.0    NaN    0.0  05db9164  fbad5c96  d356c7e6          \n",
       "4  0.0   NaN    1.0  203.0    5.0  68fd1e64  fbad5c96  d5f62b87          \n",
       "5  0.0   1.0   -1.0    0.0    0.0  05db9164            1b76cf1e          \n",
       "6  1.0   NaN   39.0    NaN  117.0  5bfa8ab5  7e0ccccf  af0809a5          \n",
       "7  0.0   NaN    0.0   66.0    7.0  05db9164            da33ebe6          \n",
       "8  1.0  10.0    1.0   66.0   27.0  05db9164  fbad5c96  ce4f7f55          \n",
       "9  0.0   NaN    1.0   16.0    7.0  68fd1e64  7e0ccccf  5e64ce5f          "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#small_toy_example.to_csv(\"toy_example.txt\",index=None)\n",
    "small_toy_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Cross Validation Grid Search for Regularization and Elastic Net Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission of Data to Kaggle Our Test Set to Kaggle To See Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggleOutput = testPredsAndLabels.select([\"id\", \"probability\"])\n",
    "kaggleOutput = kaggleOutput.rdd.map(lambda x: (int(x['id']), float(x['probability'][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Some Brief Stuff\n",
    "\n",
    "kaggleOutput = lr_test_predictions.select([\"id\", \"probability\"])\n",
    "kaggleOutput = kaggleOutput.rdd.map(lambda x: (int(x['id']), float(x['probability'][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "... Executed in 0.05407404899597168 seconds\n"
     ]
    }
   ],
   "source": [
    "# Convert train file from csv to parquet\n",
    "start = time.time()\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"probability\", FloatType(), True)])\n",
    "\n",
    "kaggleDF = sqlContext.createDataFrame(kaggleOutput, schema)\n",
    "print(f\"\\n... Executed in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "... Executed in 42.62530779838562 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "kaggleDF.write.parquet('kaggle.parquet')\n",
    "print(f\"\\n... Executed in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "... Executed in 0.035701751708984375 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "kaggleDF = sqlContext.read.parquet('kaggle.parquet')\n",
    "print(f\"\\n... Executed in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "... Executed in 14.604223251342773 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "pandasDF = kaggleDF.toPandas()\n",
    "pandasDF.columns=['id','Predicted']\n",
    "print(f\"\\n... Executed in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "... Executed in 11.946702718734741 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "pandasDF.to_csv('submission.csv', header=True, index=False)\n",
    "print(f\"\\n... Executed in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 114M/114M [01:40<00:00, 1.19MB/s]\n",
      "Successfully submitted to Display Advertising Challenge"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c criteo-display-ad-challenge -f submission.csv -m \"Baseline Submission\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
